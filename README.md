# RScraping
 
1. 파일 구성 설명

 scraping.R - 실제로 웹페이지를 긁고 거기서 데이터를 뽑아내며 데이터를 시각적으로 보여주는 코드 파일
 
 task.R - scraping.R 코드를 task scheduler에 등록하여 사이트의 갱신 시간인 1일마다 같은 작업을 수행하도록 하는 코드 파일
 
 
 
 
2. 프로젝트 상세 설명

2-1. 개요
 
 인터넷이 상당히 보급화 된 요즘 tv보다는 인터넷 방송을 어린 나이에서부터 접할 수 있고, 최근에는 인터넷 방송 영역에 발을 들이는 노년층도 증가하면서 세계적으로 인터넷 개인 방송을 지원하고 있는 플랫폼인 '트위치'가 급부상하고 있다.
 
 트위치 내에서 개인 방송을 진행하는 사람(이하, '스트리머')들 사이의 순위는 그 수익의 절대치를 나타내는 지표가 될 수는 없으나 상당 부분 연관이 되어있고, 일부 상위권의 방송 컨텐츠가 전반적인 유행의 흐름에 큰 영향을 미치기도 한다.
 
 따라서 스트리머들의 순위와 그 성장의 정도, 일정 순위 내에서 스트리머들의 국가의 분포 정도나 방송의 주요 컨텐츠를 분석하는 것은 인터넷 방송의 흐름, 곧 사람들이 선호하여 자금이 흐르는 방향을 분석하는 일에 도움이 될 수 있고, 방송을 시작하고자 하는 사람들에게는 현재 인기 있는 컨텐츠를 잡아 기반을 다지는 데에 도움이 될 수 있으리라고 판단하여 가공하기에 따라 상당히 유용한 자료가 될 수 있으리라고 생각한다.
 

2-2. 프로젝트의 진행

우선 현재 트위치 스트리머의 순위를 제공하는 사이트가 존재하는지를 확인하여 자료를 더 편하게 모을 수 있을지 확인하였다. 해당 사이트가 존재하지 않아도 동일한 내용의 프로젝트가 가능하나, 자료를 수집하는 데에 수백 번의 검색이 추가로 필요하므로 한 번 코드를 실행할 때마다 필요한 시간이 증가하게 되므로 경제적이지 않다고 판단하였다.

사이트에서 제공하는 정보는 24시간마다 업데이트가 이루어지며, 한 번 업데이트를 하면 이전 자료는 서버에 저장되지 않는다. 이는 과거에서 현재까지의 변화한 과정이 필요한 경우에 참고할 데이터가 남지 않아 어려운 상황이 될 수 있으므로 rvest 패키지를 사용하여 현재 페이지를 긁어 .html 파일로 날짜별로 나누어 저장할 수 있도록 진행하였다.

이렇게 저장한 .html 파일들에서 Rselenium 패키지로 필요한 정보를 골라내었다. 이 과정은 rvest 패키지에서 제공하는 함수로 진행하여도 무관하나 다양한 패키지를 사용해보는 것 역시 이번 프로젝트의 목적 중 하나였으므로 선택한 방법이다.

사이트는 전체 5개의 카테고리를 제공한다. 이 각각의 카테고리에서 200위까지의 스트리머 이름, 카테고리에 해당하는 지표값, 국가, 방송 태그를 읽어와 저장한다.

이렇게 저장한 데이터를 모아 각 카테고리에서 1~5위 스트리머 순위(5개), 200위까지의 국가 분포(5개), 가장 인기있는 방송 태그(1개)를 시각적으로 확인할 수 있도록 하였다. 여기서 방송 태그를 나타내는 그래프는 5개의 카테고리에서 공통적으로 1~3위를 차지하는 태그들로 선정했으며, 하나의 카테고리에서만 나타난 순위권 태그 하나는 큰 의미를 가지지 않는다고 판단하여 이를 삭제하였다.

사이트를 긁어서 저장하고 저장한 파일에서 데이터를 뽑아오는 일이 오래 걸리기도 하고, 총 20개의 페이지에서 각 페이지마다 200개의 데이터를 추출하기 때문에 이후 시각화와는 다르게 페이지 처리 속도가 매우 느리다. 따라서 추후 과거의 정보가 필요할 때 다시 파일에서 데이터를 읽는 것은 비용이 많이 드는 일로 판단하여 코드 최하단에 현재까지 읽은 데이터를 카테고리 별로 나누어 csv 파일로 저장할 수 있는 데이터를 추가하였다. 해당 파일은 사용자가 직접 열어 확인하기에도 편하고 추후 필요한 경우 R이나 다른 언어에서 다시 읽어 사용하기 매우 편리할 것으로 예상한다.


2-3. 
